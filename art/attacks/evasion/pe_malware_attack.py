# MIT License
#
# Copyright (C) The Adversarial Robustness Toolbox (ART) Authors 2018
#
# Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated
# documentation files (the "Software"), to deal in the Software without restriction, including without limitation the
# rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit
# persons to whom the Software is furnished to do so, subject to the following conditions:
#
# The above copyright notice and this permission notice shall be included in all copies or substantial portions of the
# Software.
#
# THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE
# WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
# TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
# SOFTWARE.

"""
This module implements the white-box attack _____.

| Paper link:
"""

import numpy as np
import tensorflow as tf
from art.attacks.attack import EvasionAttack
from typing import Optional, Union, Tuple, List


class MalwareGD(EvasionAttack):

    def __init__(self,
                 model_weights,
                 param_dic,
                 num_of_iterations: Optional[int] = 10,
                 lf=tf.keras.losses.CategoricalCrossentropy(from_logits=True),
                 l0: Union[float, int] = 0.1,
                 verbose: Optional[bool] = False) -> None:

        """
        :param model_weights: The weights of the trained model
        :param param_dic: a dictionary specifying some MalConv parameters.
                         'maxlen': the input size to the MalConv model
                         'input_dim': the number of discrete values, normally 257.
                         'embedding_size': size of the embedding layer. Default 8.
        :param num_of_iterations: The number of iterations to apply.
        :param lf: loss function used by the classifier. Here we will use CategoricalCrossentropy to fit into the
                   broader ART pattern of everything being multiclass. In future, change to also have BinaryCrossentropy
        :param l0: l0 bound for the attack. If less then 1 it is interpreted as a fraction of the file size.
                    If larger than 1 it is interpreted as the total number of permissible features to change.
        :param verbose: if additional information will be printed to the terminal
        """
        self.param_dic = param_dic
        self.embedding_model = self.get_embedding_model()
        self.prediction_model = self.get_prediction_model()
        self.l0 = l0
        self.total_perturbation = None
        self.num_of_iterations = num_of_iterations
        self.global_weights = model_weights
        self.lf = lf
        self.assign_weights()
        self.verbose = verbose

    def get_embedding_model(self):
        """
        One layer model to go from the raw data to the embeddings.
        """
        inp = tf.keras.layers.Input(shape=(self.param_dic['maxlen'],))
        emb = tf.keras.layers.Embedding(self.param_dic['input_dim'],
                                        self.param_dic['embedding_size'],
                                        name='embedding_layer')(inp)
        return tf.keras.Model(inputs=inp, outputs=emb)

    def get_prediction_model(self):
        """
        Model going from embeddings to predictions so we can easily optimise the embedding malware embedding.
        Needs to have the same structure as the target model.
        Populated here with "standard" parameters.
        """
        inp = tf.keras.layers.Input(shape=(self.param_dic['maxlen'], self.param_dic['embedding_size'],))
        filt = tf.keras.layers.Conv1D(filters=128, kernel_size=500,
                                      strides=500, use_bias=True, activation='relu',
                                      padding='valid', name='filt_layer')(inp)
        attn = tf.keras.layers.Conv1D(filters=128, kernel_size=500,
                                      strides=500, use_bias=True, activation='sigmoid',
                                      padding='valid', name='attn_layer')(inp)
        gated = tf.keras.layers.Multiply()([filt, attn])
        feat = tf.keras.layers.GlobalMaxPooling1D()(gated)
        dense = tf.keras.layers.Dense(128, activation='relu', name='dense_layer')(feat)
        output = tf.keras.layers.Dense(2, name='output_layer')(dense)
        return tf.keras.Model(inputs=inp, outputs=[gated, output, output[:, 1]])

    def assign_weights(self):
        """
        Put the weights from the target model into the split models
        (data -> embedding model and embedding -> predictions)
        """
        embedding_weight_list = self.embedding_model.get_weights()
        prediction_model_weight_list = self.prediction_model.get_weights()

        for i, w in enumerate(self.global_weights):
            if i == 0:
                embedding_weight_list[0] = w
            else:
                prediction_model_weight_list[i-1] = w

        self.embedding_model.set_weights(embedding_weight_list)
        self.prediction_model.set_weights(prediction_model_weight_list)

    @staticmethod
    def initialise_sample(x: np.ndarray, y: np.ndarray, sample_sizes: Union[np.ndarray, "tf.Tensor"],
                          perturbation_size: np.ndarray, batch_of_slack_sizes: List[List[int]],
                          batch_of_slack_starts: List[List[int]]) -> np.ndarray:
        """
        Randomly append bytes at the end of the malware to initialise it, or if slack regions are provided,
        perturb those.

        :param x: array with input data.
        :param y: labels, after having been adjusted to account for malware which cannot support the full l0 budget.
        :param sample_sizes: the size of the original file, before it was padded to the input size required by MalConv
        :param perturbation_size: size of the perturbations in L0 terms to put at end of file
        :param batch_of_slack_sizes: list of length batch size, each element is in itself a list containing the size
                                     of the allowable slack region
        :param batch_of_slack_starts: list of length batch size, each element is in itself a list containing the start
                                      of slack region.
        :return x: array with features to be perturbed set to a random value.
        """
        for j in range(len(x)):
            if np.argmax(y[j]) == 1:
                if batch_of_slack_sizes is not None:
                    sample_slack_sizes = batch_of_slack_sizes[j]
                    sample_slack_starts = batch_of_slack_starts[j]

                    for size, start in zip(sample_slack_sizes, sample_slack_starts):
                        x[j, start:start + size] = np.random.randint(low=0, high=256, size=(1, size))
                x[j, sample_sizes[j]:sample_sizes[j]+perturbation_size[j]] = np.random.randint(low=0, high=256, size=(1, perturbation_size[j]))

        return x

    def check_valid_size(self, y: Union[np.ndarray, "tf.Tensor"], sample_sizes: Union[np.ndarray, "tf.Tensor"],
                         perturbation_size: np.ndarray) -> np.ndarray:
        """
        Checks that we can append the l0 perturbation to the malware sample and not exceed the
        maximum filesize
        We make here a new label vector with just the valid files indicated.
        :param y: labels
        :param sample_sizes: the size of the original file, before it was padded to the input size required by MalConv
        :param perturbation_size: size of the perturbations in L0 terms to put at end of file
        :return adv_label_vector: labels which indicate which malware samples have enough free features to accomodate
                                  all the adversarial perturbation.
        """

        adv_label_vector = np.zeros_like(y)
        for i in range(len(y)):
            if np.argmax(y[i]) == 1:
                if sample_sizes[i] + perturbation_size[i] <= self.param_dic['maxlen']:
                    adv_label_vector[i, 1] = 1

                    if self.verbose:
                        print('size to append on sample {} is {}'.format(i, perturbation_size[i]))
        return adv_label_vector

    def generate_mask(self, x: np.ndarray, y: np.ndarray, sample_sizes: Union[np.ndarray, "tf.Tensor"],
                      perturbation_size: np.ndarray,  batch_of_slack_sizes: List[List[int]],
                      batch_of_slack_starts: List[List[int]]) -> "tf.Tensor":
        """
        Makes a mask we apply to the gradients to control which samples in the batch are perturbed.
        :param x: array with input data.
        :param y: labels to make sure the benign files are zero masked.
        :param sample_sizes: the size of the original file, before it was padded to the input size required by MalConv
        :param perturbation_size: size of the perturbations in L0 terms to put at end of file
        :param batch_of_slack_sizes: list of length batch size, each element is in itself a list containing the size
                                     of the allowable slack region
        :param batch_of_slack_starts: list of length batch size, each element is in itself a list containing the start
                                      of slack region.
        :return mask: array with 1s on the features we will modify on this batch and 0s elsewhere.
        """
        mask = np.zeros_like(x)
        for i in range(len(x)):
            if np.argmax(y[i]) == 1:
                # if no section information was provided, append perturbation at the end of the file.
                if batch_of_slack_sizes is None:
                    mask[i, sample_sizes[i]:sample_sizes[i] + perturbation_size[i]] = 1
                else:
                    sample_slack_sizes = batch_of_slack_sizes[i]
                    sample_slack_starts = batch_of_slack_starts[i]

                    for size, start in zip(sample_slack_sizes, sample_slack_starts):
                        mask[i, start:start + size] = 1
                    mask[i, sample_sizes[i]:sample_sizes[i] + perturbation_size[i]] = 1
                # sanity check that the total number of features marked on the mask
                # does not exceed the total perturbation budget
                assert np.sum(mask[i]) == self.total_perturbation[i]
        mask = np.expand_dims(mask, axis=-1)
        # repeat it so that it matches the 8 dimesnional embedding layer
        mask = np.concatenate([mask, mask, mask, mask, mask, mask, mask, mask], axis=-1)
        mask = tf.convert_to_tensor(mask)
        mask = tf.cast(mask, dtype='float32')
        return mask

    def amplify_initial_gradient(self, embeddings: "tf.Tensor") -> "tf.Tensor":
        """
        Maximise the activation in the maxpool layer from our adversarial perturbations
        :param embeddings: embeddings produced by the data from passing it through the first embedding layer of MalConv
        :return gated_gradients: gradients wrt the pre-maxpool layer.
        """
        with tf.GradientTape() as tape:
            tape.watch(embeddings)
            gated, _, _ = self.prediction_model(embeddings)
        # gradients wrt maximising activations
        gated_gradients = tape.gradient(gated, embeddings)
        return gated_gradients

    def compute_grads_to_output(self, embeddings: "tf.Tensor",
                                labels: np.ndarray) -> Tuple["tf.Tensor", "tf.Tensor", "tf.Tensor"]:
        """
        Get gradients wrt the MalConv classification
        :param embeddings: embeddings produced by the data from passing it through the first embedding layer of MalConv
        :param labels: labels for the data, used to compute the loss.
        :return gradients: gradients wrt the target
        :return output: model prediction
        :return classification_loss: loss on batch (both adversarial and normal samples)
        """
        with tf.GradientTape() as tape:
            tape.watch(embeddings)
            gated, output, mal_output = self.prediction_model(embeddings)
            classification_loss = self.lf(labels, output)
        gradients = - tape.gradient(mal_output, embeddings)
        return gradients, output, classification_loss

    @staticmethod
    def update_embeddings(embeddings: "tf.Tensor", gradients: "tf.Tensor", mask: "tf.Tensor") -> "tf.Tensor":
        """
        Update embeddings with the supplied updates.
        :param embeddings: embeddings produced by the data from passing it through the first embedding layer of MalConv
        :param gradients: gradients to update the embeddings
        :param mask: tensor with 1s on the embeddings we modify, 0s elsewhere.
        """
        embeddings = embeddings + gradients * mask
        return embeddings

    def get_adv_malware(self, embeddings: "tf.Tensor", data: np.ndarray, labels: np.ndarray,
                        fsize: np.ndarray, perturbation_size: np.ndarray,
                        batch_of_slack_sizes: List[List[int]] = None,
                        batch_of_slack_starts: List[List[int]] = None) -> np.ndarray:
        """
        Project the adversarial example back though the closest l2 vector
        :embeddings: adversarially optimised embeddings
        :labels: labels for the data
        :fsize: size of the original malware
        :data: original data in the feature space
        :perturbation_size: size of the l0 attack to append (if any).
        :batch_of_slack_sizes: list, with each element itself being a list of the start positions of the
                               slack regions in a sample
        :batch_of_slack_starts: list, with each element itself being a list of the start positions of the
                                start of the slack regions in a sample
        """
        embedding_lookup = self.embedding_model.get_weights()[0]

        for i in range(len(fsize)):
            if np.argmax(labels[i]) == 1:
                total_diff = 0
                m = tf.constant([1, self.param_dic['input_dim'], 1], tf.int32)

                if batch_of_slack_sizes is not None:
                    sample_slack_sizes = batch_of_slack_sizes[i]
                    sample_slack_starts = batch_of_slack_starts[i]

                    for size, start in zip(sample_slack_sizes, sample_slack_starts):
                        expanded = tf.tile(tf.expand_dims(embeddings[i, start:start + size, :], axis=1), m)
                        diff = tf.norm((expanded - embedding_lookup), axis=-1)
                        diff = tf.math.argmin(diff, axis=-1)
                        data[i, start:start + size] = diff
                        total_diff += len(diff)
                expanded = tf.tile(tf.expand_dims(embeddings[i, fsize[i]:fsize[i]+perturbation_size[i], :], axis=1), m)
                diff = tf.norm((expanded - embedding_lookup), axis=-1)
                diff = tf.math.argmin(diff, axis=-1)
                data[i, fsize[i]:fsize[i]+perturbation_size[i]] = diff
                total_diff += len(diff)

                # sanity check that the total number of perturbed features does not exceed the total perturbation budget
                assert total_diff == self.total_perturbation[i]
        return data

    @staticmethod
    def pull_out_adversarial_malware(x: np.ndarray, y: np.ndarray, initial_dtype) -> Tuple[np.ndarray, np.ndarray]:
        """
        :param x: batch of data which will contain a mix of adversarial examples and unperturbed data.
        :param y: labels indicating which are valid adversarial examples or not.
        :param initial_dtype: data can be given in a few formats (uin16, float, etc) so use initial_dtype
        to make the returned sample match the original.
        """
        num_of_malware_samples = int(np.sum(y))

        # make array and allocate, much faster then appending to list and converting
        adv_x = np.zeros((num_of_malware_samples, x.shape[1]), dtype=initial_dtype)
        adv_y = np.zeros((num_of_malware_samples, y.shape[1]))
        j = 0
        for i in range(len(x)):
            if y[i, 1] == 1:
                adv_x[j] = x[i]
                adv_y[j] = y[i]
                j += 1
        return adv_x, adv_y

    def check_block_selection(self, embeddings, file_sizes, num_bytes, y_adv, preds):
        """
        Function to verify how much of the adversarial perturbation is being selected
        Not currently used.
        """
        adv_block = 0
        pre_pool_result, _, _ = self.selected_block_model(embeddings)
        selected_blocks = np.argmax(np.squeeze(pre_pool_result), axis=1)
        min_block_number = file_sizes / 500
        max_block_number = (file_sizes + num_bytes) / 500
        for k in range(len(preds)):
            if y_adv[k] == 1:
                blocks = selected_blocks[k]
                for b in range(len(blocks)):
                    if blocks[b] > min_block_number[k]:
                        if blocks[b] < max_block_number[k]:
                            adv_block += 1
        print('adv block selected ', adv_block)

    def generate(self, input_x: np.ndarray, y: Union[np.ndarray, "tf.Tensor"], sample_sizes: Union[np.ndarray, "tf.Tensor"],
                 return_only_valid_samples: bool = True,
                 batch_of_slack_sizes: List[List[int]] = None,
                 batch_of_slack_starts: List[List[int]] = None) -> Tuple[np.ndarray, np.ndarray]:
        """
        Generates the adversarial examples.

        By default we return only the samples which we could create adversarial malware from.
        I.e. the data is 1) malicous and 2) can support the assigned L0 budget.
        optionally we can return all the data in a batch, with labels indicating which are adversarial examples.

        To assign the L0 budget we go through each list in batch_of_slack_sizes and batch_of_slack_starts in order, and
        assign the budget based on the sizes given until the l0 budget is exhausted. If after all the regions marked in
        batch_of_slack_sizes and batch_of_slack_starts we have remaining l0 perturbation we assign this at the end in an
        append style attack.

        :param input_x: array with input data.
        :param y: labels to make sure the benign files are zero masked. For now two class softmax labels to fit in with
                  the broader ART approach. Later change to binary classification.
        :param sample_sizes: the size of the original file, before it was padded to the input size required by MalConv
        :param return_only_valid_samples: whether to return only the data which can be conversted into an adversarial
                                          example, or return all the data in a batch and labels indicating which samples
                                          are adversarial or not.
        :param batch_of_slack_sizes: list of length batch size, each element is in itself a list containing
                                     the size of the allowable slack region
        :param batch_of_slack_starts: list of length batch size, each element is in itself a list containing
                                      the start of slack region.
        """

        initial_dtype = input_x.dtype
        # make copy so original data is not modified.
        x = input_x.copy()

        if np.sum(y[:, 1]) == 0:
            # no adversarial samples are in this batch
            return x, y

        perturbation_size = []
        for sample_size in sample_sizes:
            if self.l0 < 1:  # l0 is a fraction of the filesize
                perturbation_size.append(int(sample_size * self.l0))
            else:  # or l0 is interpreted as total perturbation size
                perturbation_size.append(int(self.l0))
        perturbation_size = np.asarray(perturbation_size)
        self.total_perturbation = np.copy(perturbation_size)
        # reduce the perturbation we add at the end if we have slack regions
        if batch_of_slack_sizes is not None:
            for i in range(len(perturbation_size)):
                section_sizes = batch_of_slack_sizes[i]

                for j in range(len(section_sizes)):
                    size = section_sizes[j]

                    if perturbation_size[i] - size > 0:
                        if self.verbose:
                            print('on sample {} allocate {} in slack region'.format(i, size))
                        perturbation_size[i] = perturbation_size[i] - size
                    else:  # run out of l0 budget.
                        excess = np.abs(perturbation_size[i] - size)  # amount of overspill
                        perturbation_size[i] = perturbation_size[i] - size
                        section_sizes[j] = size - excess
                        # update the slack sizes
                        if self.verbose:
                            print('on sample {} ran out of l0, update to {} from {}'.format(i, section_sizes[j], size))
                        batch_of_slack_sizes[i] = section_sizes
                        perturbation_size[i] = 0
            perturbation_size = np.where(perturbation_size < 0, 0, perturbation_size)

        y = self.check_valid_size(y, sample_sizes, perturbation_size)

        x = self.initialise_sample(x, y, sample_sizes, perturbation_size,
                                   batch_of_slack_sizes=batch_of_slack_sizes,
                                   batch_of_slack_starts=batch_of_slack_starts)

        mask = self.generate_mask(x, y, sample_sizes, perturbation_size,
                                  batch_of_slack_sizes=batch_of_slack_sizes, batch_of_slack_starts=batch_of_slack_starts)

        embeddings = self.embedding_model(x)

        gradients = self.amplify_initial_gradient(embeddings)

        embeddings = self.update_embeddings(embeddings, gradients, mask)

        for _ in range(self.num_of_iterations):
            gradients, _, _ = self.compute_grads_to_output(embeddings, y)
            embeddings = self.update_embeddings(embeddings, gradients, mask)

        x = self.get_adv_malware(embeddings=embeddings,
                                 data=x,
                                 labels=y,
                                 fsize=sample_sizes,
                                 perturbation_size=perturbation_size,
                                 batch_of_slack_sizes=batch_of_slack_sizes,
                                 batch_of_slack_starts=batch_of_slack_starts)

        if return_only_valid_samples:
            x, y = self.pull_out_adversarial_malware(x, y, initial_dtype)

        return x, y

    @staticmethod
    def process_file(filepath: str, padding_char: Optional[int] = 256,
                     maxlen: Optional[int] = 2**20) -> Tuple[np.ndarray, int]:

        """
        Go from raw file to numpy array.

        :param maxlen: maximum size of the file processed by the model. Currently set to 1MB
        :param padding_char: char to use to pad the input if it is shorter then maxlen
        :return b: numpy array of the PE file
        :return size_of_original_file: size of the PE file
        """

        f = open(filepath, "rb").read()
        size_of_original_file = len(f)

        b = np.ones((maxlen,), dtype=np.uint16) * padding_char
        bytez = np.frombuffer(f[:maxlen], dtype=np.uint8)
        b[:len(bytez)] = bytez

        return b, size_of_original_file

    @staticmethod
    def get_peinfo(filepath: str, save_to_json_path: Optional[str] = None) -> Tuple[List[int], List[int]]:
        """
       Given a PE file we extract out the section information to determine the slack regions in the file.
       We return two lists 1) with the start location of the slack regions and 2) with the size of the slack region.
       we are using the lief library (https://github.com/lief-project/LIEF) to manipulate the PE file.

       :param filepath: path to file we want to analyse with pedump and get the section information.
       :param save_to_json_path: (Optional) if we want to save the results of pedump to a json file, provide the path.
       :return: two lists, one with the slack start positions (start_of_slack)
                and the other with the slack sizes (size_of_slack)
       """

        import lief
        import json

        size_of_slack = []
        start_of_slack = []

        cleaned_dump = {}

        binary = lief.parse(filepath)
        for section in binary.sections:
            section_info = {}
            slack = section.sizeof_raw_data - section.virtual_size
            section_info['PointerToRawData'] = section.pointerto_raw_data
            section_info['VirtualAddress'] = section.virtual_size
            section_info['SizeOfRawData'] = section.sizeof_raw_data
            cleaned_dump[section.name] = section_info
            if slack > 0:
                size_of_slack.append(slack)
                start_of_slack.append(section.pointerto_raw_data + section.virtual_size)

        if save_to_json_path is not None:
            with open(save_to_json_path, 'w') as outfile:
                json.dump(cleaned_dump, outfile, indent=4, sort_keys=True)

        return size_of_slack, start_of_slack

    @staticmethod
    def get_peinfo_with_pedump(filepath: str, save_to_json_path: Optional[str] = None) -> Tuple[List[int], List[int]]:

        """
        Alternative method using pedump to get the section information

        Given a PE file we extract out the section information to determine the slack regions in the file.
        We return two lists 1) with the start location of the slack regions and 2) with the size of the slack region.
        Here we are using pedump to get the information (https://github.com/zed-0xff/pedump)
        :param filepath: path to file we want to analyse with pedump and get the section information.
        :param save_to_json_path: (Optional) if we want to save the results of pedump to a json file, provide the path.
        :return: two lists, one with the slack start positions (start_of_slack)
                 and the other with the slack sizes (size_of_slack)
        """
        import os
        import json

        cmd = 'pedump --format json --sections ' + filepath
        result = os.popen(cmd).read()

        # If the PE file could not be read we return empty lists
        if not result:
            return [], []

        # parse the output of pedump to make it into a nice json
        result = result[19:]
        tmp = json.loads(result)

        cleaned_dump = {}
        for entry in tmp:
            entry = entry.replace(' ', '')
            entry = entry.split(",")

            section_name = entry[0]
            entry = entry[1:]
            section_info = {}
            for sec in entry:
                sec = sec.split('=')
                section_info[sec[0]] = sec[1]

            cleaned_dump[section_name] = section_info

        if save_to_json_path is not None:
            with open(save_to_json_path, 'w') as outfile:
                json.dump(cleaned_dump, outfile, indent=4, sort_keys=True)

        size_of_slack = []
        start_of_slack = []
        for section_name, section in cleaned_dump.items():
            raw_data_size = int(section['SizeOfRawData'])
            virtual_data_size = int(section['VirtualSize'])
            raw_address = int(section['PointerToRawData'])

            slack = raw_data_size - virtual_data_size

            if slack > 0:
                size_of_slack.append(slack)
                start_of_slack.append(raw_address + virtual_data_size)

        return size_of_slack, start_of_slack

    def insert_section(self, filepath, padding_char: Optional[int] = 256,
                       maxlen: Optional[int] = 2**20, bytes_to_assign: Optional[int] = None):
        """
        create a new section in a PE file that the attacker can perturb to create an adversarial example.
        we are using the lief library (https://github.com/lief-project/LIEF) to manipulate the PE file.

        :param filepath: path to file we want to analyse with pedump and get the section information.
        :param padding_char:
        :param maxlen:
        :param bytes_to_assign: (Optional) how many bytes we wish to specify when inserting a new section. If unspecified
                                the whole l0 budget will be used on a single section.

        :return b: executable with section inserted and turned into a numpy array of the appropriate size
        :return information_on_section.pointerto_raw_data: the start of the inserted section
        :return information_on_section.virtual_size: size of the inserted section
        :return size_of_slack: size of slack regions in this executable (including from the section we just inserted)
        :return start_of_slack: start of slack regions in this executable (including from the section we just inserted)
        """
        import lief
        import random

        binary = lief.parse(filepath)

        # new_section = lief.PE.Section("".join(chr(random.randrange(ord('.'), ord('z'))) for _ in range(6)))
        new_section_name = ".mysec"
        new_section = lief.PE.Section(new_section_name)

        if bytes_to_assign is None:
            new_section.content = [random.randint(0, 255) for _ in range(self.l0)]
        else:
            new_section.content = [random.randint(0, 255) for _ in range(bytes_to_assign)]


        # we add the new section at the end of the existing sections
        new_section.virtual_address = max([section.virtual_address + section.size for section in binary.sections])

        binary.add_section(new_section, random.choice([lief.PE.SECTION_TYPES.BSS,
                                                       lief.PE.SECTION_TYPES.DATA,
                                                       lief.PE.SECTION_TYPES.EXPORT,
                                                       lief.PE.SECTION_TYPES.IDATA,
                                                       lief.PE.SECTION_TYPES.RELOCATION,
                                                       lief.PE.SECTION_TYPES.RESOURCE,
                                                       lief.PE.SECTION_TYPES.TEXT,
                                                       lief.PE.SECTION_TYPES.TLS_,
                                                       lief.PE.SECTION_TYPES.UNKNOWN]))

        information_on_section = binary.get_section(new_section_name)

        size_of_slack = []
        start_of_slack = []
        for section in binary.sections:
            slack = section.sizeof_raw_data - section.virtual_size
            if slack > 0:
                size_of_slack.append(slack)
                start_of_slack.append(section.pointerto_raw_data + section.virtual_size)

        builder = lief.PE.Builder(binary)
        builder.build()

        manipulated_file = np.array(builder.get_build(), dtype=np.uint8)

        b = np.ones((maxlen,), dtype=np.uint16) * padding_char
        # todo checks for files which are too big
        if len(manipulated_file > maxlen):
            manipulated_file = manipulated_file[:maxlen]

        b[:len(manipulated_file)] = manipulated_file[:len(manipulated_file)]

        return b, len(manipulated_file), \
               information_on_section.pointerto_raw_data, \
               information_on_section.virtual_size, \
               size_of_slack, \
               start_of_slack