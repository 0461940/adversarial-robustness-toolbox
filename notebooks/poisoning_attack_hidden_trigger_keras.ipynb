{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temporary directory not created yet\n",
      "Temporary directory: /tmp/tmpu6vakty8\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "from os.path import abspath\n",
    "\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import tempfile\n",
    "# Create a temporary directory for the model checkpoint. Remove the exising one if this cell is rerun\n",
    "try:\n",
    "    temp_model_dir.cleanup()\n",
    "except NameError:\n",
    "    print(\"Temporary directory not created yet\")\n",
    "finally:\n",
    "    temp_model_dir = tempfile.TemporaryDirectory() \n",
    "    print(\"Temporary directory:\", temp_model_dir.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset and Model\n",
    "We will load the CIFAR10 dataset and a pre-trained alexnet model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.utils import load_dataset\n",
    "import numpy as np\n",
    "(x_train, y_train), (x_test, y_test), min_, max_ = load_dataset('cifar10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 30, 30, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 15, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 13, 13, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               1180160   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 1,250,858\n",
      "Trainable params: 1,250,858\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-03 18:27:02.355404: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-03 18:27:02.364165: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-03 18:27:02.365018: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "from tensorflow.keras import datasets, layers, models, losses\n",
    "\n",
    "num_classes=10\n",
    "feature_size=4096\n",
    "model = models.Sequential()\n",
    "\n",
    "# Create Keras convolutional neural network - basic architecture from Keras examples\n",
    "# Source here: https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn.py\n",
    "model.add(layers.Conv2D(32, (3, 3), padding=\"same\", activation='relu', input_shape=x_train.shape[1:]))\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(layers.Dropout(0.25))\n",
    "\n",
    "model.add(layers.Conv2D(64, (3, 3), padding=\"same\", activation='relu'))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(layers.Dropout(0.25))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(layers.Dense(10))\n",
    "\n",
    "model.compile(loss=losses.CategoricalCrossentropy(from_logits=True), optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    \n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.estimators.classification import KerasClassifier\n",
    "\n",
    "classifier = KerasClassifier(clip_values=(min_, max_), model=model, use_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-03 18:27:04.352847: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-02-03 18:27:04.354288: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-03 18:27:04.355195: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-03 18:27:04.356000: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-03 18:27:04.948807: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-03 18:27:04.949685: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-03 18:27:04.950564: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-03 18:27:04.951356: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15405 MB memory:  -> device: 0, name: NVIDIA Tesla P100-PCIE-16GB, pci bus id: 0000:00:06.0, compute capability: 6.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-03 18:27:05.941933: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8201\n",
      "2022-02-03 18:27:06.283257: I tensorflow/core/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000/50000 [==============================] - 6s 115us/sample - loss: 1.6410 - accuracy: 0.3998\n",
      "Epoch 2/10\n",
      "50000/50000 [==============================] - 4s 82us/sample - loss: 1.2271 - accuracy: 0.5628\n",
      "Epoch 3/10\n",
      "50000/50000 [==============================] - 4s 80us/sample - loss: 1.0572 - accuracy: 0.6285\n",
      "Epoch 4/10\n",
      "50000/50000 [==============================] - 4s 81us/sample - loss: 0.9375 - accuracy: 0.6683\n",
      "Epoch 5/10\n",
      "50000/50000 [==============================] - 4s 80us/sample - loss: 0.8587 - accuracy: 0.6966\n",
      "Epoch 6/10\n",
      "50000/50000 [==============================] - 4s 81us/sample - loss: 0.7970 - accuracy: 0.7222\n",
      "Epoch 7/10\n",
      "50000/50000 [==============================] - 4s 81us/sample - loss: 0.7459 - accuracy: 0.7380\n",
      "Epoch 8/10\n",
      "50000/50000 [==============================] - 4s 81us/sample - loss: 0.7049 - accuracy: 0.7518\n",
      "Epoch 9/10\n",
      "50000/50000 [==============================] - 4s 81us/sample - loss: 0.6634 - accuracy: 0.7675\n",
      "Epoch 10/10\n",
      "50000/50000 [==============================] - 4s 82us/sample - loss: 0.6268 - accuracy: 0.7806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-03 18:27:47.763566: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-03 18:27:47.763976: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-03 18:27:47.764242: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-03 18:27:47.764556: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-03 18:27:47.764897: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-02-03 18:27:47.765072: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15405 MB memory:  -> device: 0, name: NVIDIA Tesla P100-PCIE-16GB, pci bus id: 0000:00:06.0, compute capability: 6.0\n",
      "2022-02-03 18:27:48.313437: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpu6vakty8/keras_model/assets\n"
     ]
    }
   ],
   "source": [
    "#classifier.fit(x_train, y_train, nb_epochs=200, batch_size=128, verbose=True)\n",
    "classifier.fit(x_train, y_train, nb_epochs=10, batch_size=128)\n",
    "model.save(temp_model_dir.name + '/keras_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on benign test examples: 77.84%\n"
     ]
    }
   ],
   "source": [
    "predictions = classifier.predict(x_test)\n",
    "accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1)) / len(y_test)\n",
    "print(\"Accuracy on benign test examples: {}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the Poison\n",
    "We now will generate the poison using the hidden trigger backdoor attack. First, we define the target and source classes as well as the backdoor trigger. The target class will be the class we want to insert poisoned data into. The source class will be the class we will add a trigger to in order to cause misclassification into the target.\n",
    "\n",
    "The backdoor trigger will be a small image patch inserted into the source class images. At test time, we should be able to use this trigger to cause the classifier to misclassify source class images with the trigger added as the target class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from art.attacks.poisoning.backdoor_attack import PoisoningAttackBackdoor\n",
    "target = np.array([0,0,0,0,1,0,0,0,0,0])\n",
    "source = np.array([0,0,0,1,0,0,0,0,0,0])\n",
    "\n",
    "# Backdoor Trigger Parameters\n",
    "patch_size = 8\n",
    "x_shift = 32 - patch_size - 5\n",
    "y_shift = 32 - patch_size - 5\n",
    "\n",
    "# Define the backdoor poisoning object. Calling backdoor.poison(x) will insert the trigger into x.\n",
    "from art.attacks.poisoning import perturbations\n",
    "def mod(x):\n",
    "    original_dtype = x.dtype\n",
    "    x = perturbations.insert_image(x, backdoor_path=\"../utils/data/backdoors/htbd.png\",\n",
    "                                   channels_first=False, random=False, x_shift=x_shift, y_shift=y_shift,\n",
    "                                   size=(patch_size,patch_size), mode='RGB', blend=1)\n",
    "    return x.astype(original_dtype)\n",
    "backdoor = PoisoningAttackBackdoor(mod)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we run the attack. `eps` controls how much the target images can be perturbed with respect to an l-infinity distance. `feature_layer` dicates with layer's output will be used to define the attack's loss. It can either be the name of the layer or the layer index according to the ART estimator. `poison_percent` controls how many poisoned samples will be generated based on the size of the input data.\n",
    "\n",
    "The attack will return poisoned inputs of the target class and the indicies in the data that those poisoned inputs should replace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8659e1f45ce747a4a5a16c645e0d7e46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Hidden Trigger:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0 | batch: 0 | i:     0 | LR: 0.01000 | Loss Val: 4063.301 | Loss Avg: 4063.301\n",
      "Epoch:  0 | batch: 0 | i:   100 | LR: 0.01000 | Loss Val: 221.664 | Loss Avg: 367.815\n",
      "Epoch:  0 | batch: 0 | i:   200 | LR: 0.01000 | Loss Val: 204.376 | Loss Avg: 290.393\n",
      "Epoch:  0 | batch: 0 | i:   300 | LR: 0.01000 | Loss Val: 197.959 | Loss Avg: 260.335\n",
      "Epoch:  0 | batch: 0 | i:   400 | LR: 0.01000 | Loss Val: 196.285 | Loss Avg: 244.444\n",
      "Epoch:  0 | batch: 0 | i:   500 | LR: 0.01000 | Loss Val: 194.839 | Loss Avg: 235.321\n",
      "Epoch:  0 | batch: 0 | i:   600 | LR: 0.01000 | Loss Val: 189.284 | Loss Avg: 228.162\n",
      "Epoch:  0 | batch: 0 | i:   700 | LR: 0.01000 | Loss Val: 193.144 | Loss Avg: 223.032\n",
      "Epoch:  0 | batch: 0 | i:   800 | LR: 0.01000 | Loss Val: 190.913 | Loss Avg: 218.994\n",
      "Epoch:  0 | batch: 0 | i:   900 | LR: 0.01000 | Loss Val: 191.389 | Loss Avg: 215.933\n",
      "Epoch:  0 | batch: 0 | i:  1000 | LR: 0.00100 | Loss Val: 195.823 | Loss Avg: 213.770\n",
      "Epoch:  0 | batch: 0 | i:  1100 | LR: 0.00100 | Loss Val: 181.642 | Loss Avg: 210.956\n",
      "Epoch:  0 | batch: 0 | i:  1200 | LR: 0.00100 | Loss Val: 181.019 | Loss Avg: 208.486\n",
      "Epoch:  0 | batch: 0 | i:  1300 | LR: 0.00100 | Loss Val: 180.706 | Loss Avg: 206.361\n",
      "Epoch:  0 | batch: 0 | i:  1400 | LR: 0.00100 | Loss Val: 180.512 | Loss Avg: 204.523\n",
      "Epoch:  0 | batch: 0 | i:  1500 | LR: 0.00100 | Loss Val: 180.360 | Loss Avg: 202.918\n",
      "Epoch:  0 | batch: 0 | i:  1600 | LR: 0.00100 | Loss Val: 180.236 | Loss Avg: 201.504\n",
      "Epoch:  0 | batch: 0 | i:  1700 | LR: 0.00100 | Loss Val: 180.142 | Loss Avg: 200.251\n",
      "Epoch:  0 | batch: 0 | i:  1800 | LR: 0.00100 | Loss Val: 180.056 | Loss Avg: 199.132\n",
      "Epoch:  0 | batch: 0 | i:  1900 | LR: 0.00100 | Loss Val: 179.983 | Loss Avg: 198.127\n",
      "Epoch:  0 | batch: 0 | i:  2000 | LR: 0.00010 | Loss Val: 179.919 | Loss Avg: 197.218\n",
      "Epoch:  0 | batch: 0 | i:  2100 | LR: 0.00010 | Loss Val: 179.895 | Loss Avg: 196.394\n",
      "Epoch:  0 | batch: 0 | i:  2200 | LR: 0.00010 | Loss Val: 179.889 | Loss Avg: 195.644\n",
      "Epoch:  0 | batch: 0 | i:  2300 | LR: 0.00010 | Loss Val: 179.883 | Loss Avg: 194.960\n",
      "Epoch:  0 | batch: 0 | i:  2400 | LR: 0.00010 | Loss Val: 179.877 | Loss Avg: 194.331\n",
      "Epoch:  0 | batch: 0 | i:  2500 | LR: 0.00010 | Loss Val: 179.872 | Loss Avg: 193.753\n",
      "Epoch:  0 | batch: 0 | i:  2600 | LR: 0.00010 | Loss Val: 179.866 | Loss Avg: 193.220\n",
      "Epoch:  0 | batch: 0 | i:  2700 | LR: 0.00010 | Loss Val: 179.860 | Loss Avg: 192.725\n",
      "Epoch:  0 | batch: 0 | i:  2800 | LR: 0.00010 | Loss Val: 179.855 | Loss Avg: 192.266\n",
      "Epoch:  0 | batch: 0 | i:  2900 | LR: 0.00010 | Loss Val: 179.850 | Loss Avg: 191.838\n",
      "Max_Loss: 179.84467053857225\n",
      "Epoch:  0 | batch: 1 | i:     0 | LR: 0.01000 | Loss Val: 3501.437 | Loss Avg: 192.545\n",
      "Epoch:  0 | batch: 1 | i:   100 | LR: 0.01000 | Loss Val: 267.612 | Loss Avg: 198.708\n",
      "Epoch:  0 | batch: 1 | i:   200 | LR: 0.01000 | Loss Val: 253.145 | Loss Avg: 200.609\n",
      "Epoch:  0 | batch: 1 | i:   300 | LR: 0.01000 | Loss Val: 253.838 | Loss Avg: 202.073\n",
      "Epoch:  0 | batch: 1 | i:   400 | LR: 0.01000 | Loss Val: 227.983 | Loss Avg: 203.140\n",
      "Epoch:  0 | batch: 1 | i:   500 | LR: 0.01000 | Loss Val: 229.490 | Loss Avg: 203.791\n",
      "Epoch:  0 | batch: 1 | i:   600 | LR: 0.01000 | Loss Val: 222.671 | Loss Avg: 204.378\n",
      "Epoch:  0 | batch: 1 | i:   700 | LR: 0.01000 | Loss Val: 221.752 | Loss Avg: 204.839\n",
      "Epoch:  0 | batch: 1 | i:   800 | LR: 0.01000 | Loss Val: 220.624 | Loss Avg: 205.295\n",
      "Epoch:  0 | batch: 1 | i:   900 | LR: 0.01000 | Loss Val: 218.830 | Loss Avg: 205.695\n",
      "Epoch:  0 | batch: 1 | i:  1000 | LR: 0.00100 | Loss Val: 224.361 | Loss Avg: 206.077\n",
      "Epoch:  0 | batch: 1 | i:  1100 | LR: 0.00100 | Loss Val: 216.629 | Loss Avg: 206.344\n",
      "Epoch:  0 | batch: 1 | i:  1200 | LR: 0.00100 | Loss Val: 216.366 | Loss Avg: 206.586\n",
      "Epoch:  0 | batch: 1 | i:  1300 | LR: 0.00100 | Loss Val: 216.295 | Loss Avg: 206.813\n",
      "Epoch:  0 | batch: 1 | i:  1400 | LR: 0.00100 | Loss Val: 216.218 | Loss Avg: 207.027\n",
      "Epoch:  0 | batch: 1 | i:  1500 | LR: 0.00100 | Loss Val: 216.109 | Loss Avg: 207.230\n",
      "Epoch:  0 | batch: 1 | i:  1600 | LR: 0.00100 | Loss Val: 216.118 | Loss Avg: 207.423\n",
      "Epoch:  0 | batch: 1 | i:  1700 | LR: 0.00100 | Loss Val: 215.986 | Loss Avg: 207.607\n",
      "Epoch:  0 | batch: 1 | i:  1800 | LR: 0.00100 | Loss Val: 216.032 | Loss Avg: 207.782\n",
      "Epoch:  0 | batch: 1 | i:  1900 | LR: 0.00100 | Loss Val: 215.887 | Loss Avg: 207.949\n",
      "Epoch:  0 | batch: 1 | i:  2000 | LR: 0.00010 | Loss Val: 215.956 | Loss Avg: 208.108\n",
      "Epoch:  0 | batch: 1 | i:  2100 | LR: 0.00010 | Loss Val: 215.839 | Loss Avg: 208.260\n",
      "Epoch:  0 | batch: 1 | i:  2200 | LR: 0.00010 | Loss Val: 215.825 | Loss Avg: 208.405\n",
      "Epoch:  0 | batch: 1 | i:  2300 | LR: 0.00010 | Loss Val: 215.829 | Loss Avg: 208.545\n",
      "Epoch:  0 | batch: 1 | i:  2400 | LR: 0.00010 | Loss Val: 215.818 | Loss Avg: 208.680\n",
      "Epoch:  0 | batch: 1 | i:  2500 | LR: 0.00010 | Loss Val: 215.819 | Loss Avg: 208.810\n",
      "Epoch:  0 | batch: 1 | i:  2600 | LR: 0.00010 | Loss Val: 215.811 | Loss Avg: 208.935\n",
      "Epoch:  0 | batch: 1 | i:  2700 | LR: 0.00010 | Loss Val: 215.811 | Loss Avg: 209.056\n",
      "Epoch:  0 | batch: 1 | i:  2800 | LR: 0.00010 | Loss Val: 215.804 | Loss Avg: 209.172\n",
      "Epoch:  0 | batch: 1 | i:  2900 | LR: 0.00010 | Loss Val: 215.802 | Loss Avg: 209.284\n",
      "Max_Loss: 215.79961807387826\n",
      "Epoch:  0 | batch: 2 | i:     0 | LR: 0.01000 | Loss Val: 3002.525 | Loss Avg: 209.857\n",
      "Epoch:  0 | batch: 2 | i:   100 | LR: 0.01000 | Loss Val: 225.674 | Loss Avg: 211.348\n",
      "Epoch:  0 | batch: 2 | i:   200 | LR: 0.01000 | Loss Val: 215.287 | Loss Avg: 211.483\n",
      "Epoch:  0 | batch: 2 | i:   300 | LR: 0.01000 | Loss Val: 213.701 | Loss Avg: 211.538\n",
      "Epoch:  0 | batch: 2 | i:   400 | LR: 0.01000 | Loss Val: 211.690 | Loss Avg: 211.559\n",
      "Epoch:  0 | batch: 2 | i:   500 | LR: 0.01000 | Loss Val: 211.626 | Loss Avg: 211.571\n",
      "Epoch:  0 | batch: 2 | i:   600 | LR: 0.01000 | Loss Val: 210.122 | Loss Avg: 211.562\n",
      "Epoch:  0 | batch: 2 | i:   700 | LR: 0.01000 | Loss Val: 209.903 | Loss Avg: 211.568\n",
      "Epoch:  0 | batch: 2 | i:   800 | LR: 0.01000 | Loss Val: 210.887 | Loss Avg: 211.549\n",
      "Epoch:  0 | batch: 2 | i:   900 | LR: 0.01000 | Loss Val: 212.262 | Loss Avg: 211.538\n",
      "Epoch:  0 | batch: 2 | i:  1000 | LR: 0.00100 | Loss Val: 210.077 | Loss Avg: 211.521\n",
      "Epoch:  0 | batch: 2 | i:  1100 | LR: 0.00100 | Loss Val: 208.902 | Loss Avg: 211.485\n",
      "Epoch:  0 | batch: 2 | i:  1200 | LR: 0.00100 | Loss Val: 208.854 | Loss Avg: 211.449\n",
      "Epoch:  0 | batch: 2 | i:  1300 | LR: 0.00100 | Loss Val: 208.825 | Loss Avg: 211.413\n",
      "Epoch:  0 | batch: 2 | i:  1400 | LR: 0.00100 | Loss Val: 208.805 | Loss Avg: 211.378\n",
      "Epoch:  0 | batch: 2 | i:  1500 | LR: 0.00100 | Loss Val: 208.789 | Loss Avg: 211.344\n",
      "Epoch:  0 | batch: 2 | i:  1600 | LR: 0.00100 | Loss Val: 208.775 | Loss Avg: 211.310\n",
      "Epoch:  0 | batch: 2 | i:  1700 | LR: 0.00100 | Loss Val: 208.763 | Loss Avg: 211.277\n",
      "Epoch:  0 | batch: 2 | i:  1800 | LR: 0.00100 | Loss Val: 208.752 | Loss Avg: 211.245\n",
      "Epoch:  0 | batch: 2 | i:  1900 | LR: 0.00100 | Loss Val: 208.742 | Loss Avg: 211.213\n",
      "Epoch:  0 | batch: 2 | i:  2000 | LR: 0.00010 | Loss Val: 208.733 | Loss Avg: 211.182\n",
      "Epoch:  0 | batch: 2 | i:  2100 | LR: 0.00010 | Loss Val: 208.727 | Loss Avg: 211.152\n",
      "Epoch:  0 | batch: 2 | i:  2200 | LR: 0.00010 | Loss Val: 208.726 | Loss Avg: 211.122\n",
      "Epoch:  0 | batch: 2 | i:  2300 | LR: 0.00010 | Loss Val: 208.725 | Loss Avg: 211.093\n",
      "Epoch:  0 | batch: 2 | i:  2400 | LR: 0.00010 | Loss Val: 208.724 | Loss Avg: 211.065\n",
      "Epoch:  0 | batch: 2 | i:  2500 | LR: 0.00010 | Loss Val: 208.724 | Loss Avg: 211.038\n",
      "Epoch:  0 | batch: 2 | i:  2600 | LR: 0.00010 | Loss Val: 208.723 | Loss Avg: 211.011\n",
      "Epoch:  0 | batch: 2 | i:  2700 | LR: 0.00010 | Loss Val: 208.722 | Loss Avg: 210.984\n",
      "Epoch:  0 | batch: 2 | i:  2800 | LR: 0.00010 | Loss Val: 208.721 | Loss Avg: 210.959\n",
      "Epoch:  0 | batch: 2 | i:  2900 | LR: 0.00010 | Loss Val: 208.720 | Loss Avg: 210.934\n",
      "Max_Loss: 208.71929555252427\n",
      "Number of poison samples generated: 75\n"
     ]
    }
   ],
   "source": [
    "#from art.attacks.poisoning import HiddenTriggerBackdoor\n",
    "%run Hidden_Trigger_keras.ipynb\n",
    "poison_attack = HiddenTriggerBackdoorKeras(classifier, eps=16/255, target=target, source=source, feature_layer=9, backdoor=backdoor, learning_rate=0.01, decay_coeff = .1, decay_iter = 1000, max_iter=3000, batch_size=25, poison_percent=.015)\n",
    "\n",
    "poison_data, poison_indices = poison_attack.poison(x_train, y_train)\n",
    "print(\"Number of poison samples generated:\", len(poison_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetune the Model\n",
    "Now, we must finetune the model using the poisoned data and a small number of clean training inputs.  Here, we randomly select an equal number of training inputs from each of the classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create finetuning dataset\n",
    "dataset_size = 2500\n",
    "num_classes = 10\n",
    "num_per_class = dataset_size/num_classes\n",
    "\n",
    "poison_dataset_inds = []\n",
    "\n",
    "for i in range(num_classes):\n",
    "    class_inds = np.where(np.argmax(y_train,axis=1) == i)[0]\n",
    "    num_select = int(num_per_class)\n",
    "    if np.argmax(target) == i:\n",
    "        num_select = int(num_select - min(num_per_class,len(poison_data)))\n",
    "        poison_dataset_inds.append(poison_indices)\n",
    "        \n",
    "    if num_select != 0:\n",
    "        poison_dataset_inds.append(np.random.choice(class_inds, num_select, replace=False))\n",
    "    \n",
    "poison_dataset_inds = np.concatenate(poison_dataset_inds)\n",
    "\n",
    "poison_x = np.copy(x_train)\n",
    "poison_x[poison_indices] = poison_data\n",
    "poison_x = poison_x[poison_dataset_inds]\n",
    "\n",
    "poison_y = np.copy(y_train)[poison_dataset_inds]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 30, 30, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 15, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 13, 13, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               1180160   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 1,250,858\n",
      "Trainable params: 0\n",
      "Non-trainable params: 1,250,858\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model(temp_model_dir.name + '/keras_model')\n",
    "model.trainable = False\n",
    "model.compile(loss=losses.CategoricalCrossentropy(from_logits=True), optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on benign test examples: 77.84%\n"
     ]
    }
   ],
   "source": [
    "classifier = KerasClassifier(clip_values=(min_, max_), model=model)\n",
    "predictions = classifier.predict(x_test)\n",
    "accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1)) / len(y_test)\n",
    "print(\"Accuracy on benign test examples: {}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_input (InputLayer)    [(None, 32, 32, 3)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 32, 32, 32)        896       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 30, 30, 32)        9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 15, 15, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 15, 15, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 13, 13, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 6, 6, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 2304)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 512)               1180160   \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                5130      \n",
      "=================================================================\n",
      "Total params: 1,250,858\n",
      "Trainable params: 5,130\n",
      "Non-trainable params: 1,245,728\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "finetune_model = tf.keras.layers.Dense(10)(model.layers[-2].output)\n",
    "finetune_model = tf.keras.Model(inputs=model.inputs, outputs=finetune_model)\n",
    "finetune_model.summary()\n",
    "\n",
    "lr = 0.5\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "#optimizer = tf.keras.optimizers.SGD(learning_rate=lr, momentum=0.9)\n",
    "\n",
    "finetune_model.compile(loss=losses.CategoricalCrossentropy(from_logits=True), optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "finetune_classifier = KerasClassifier(clip_values=(min_, max_), model=finetune_model, use_logits=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on benign test examples: 6.41%\n",
      "Accuracy on benign trigger test examples: 6.1%\n",
      "Accuracy on poison trigger test examples: 6.6000000000000005%\n",
      "Success on poison trigger test examples: 2.5%\n",
      "\n",
      "Training Epoch 0\n",
      "Train on 2500 samples\n",
      "2500/2500 [==============================] - 0s 101us/sample - loss: 7.3075 - accuracy: 0.6484\n",
      "Accuracy on benign test examples: 72.97%\n",
      "Accuracy on benign trigger test examples: 50.0%\n",
      "Accuracy on poison trigger test examples: 33.6%\n",
      "Success on poison trigger test examples: 18.9%\n",
      "\n",
      "Training Epoch 1\n",
      "Train on 2500 samples\n",
      "2500/2500 [==============================] - 0s 87us/sample - loss: 6.5810 - accuracy: 0.7400\n",
      "Accuracy on benign test examples: 75.03%\n",
      "Accuracy on benign trigger test examples: 54.900000000000006%\n",
      "Accuracy on poison trigger test examples: 34.300000000000004%\n",
      "Success on poison trigger test examples: 28.999999999999996%\n",
      "\n",
      "Training Epoch 2\n",
      "Train on 2500 samples\n",
      "2500/2500 [==============================] - 0s 91us/sample - loss: 6.8730 - accuracy: 0.7332\n",
      "Accuracy on benign test examples: 75.14%\n",
      "Accuracy on benign trigger test examples: 55.300000000000004%\n",
      "Accuracy on poison trigger test examples: 34.699999999999996%\n",
      "Success on poison trigger test examples: 29.099999999999998%\n",
      "\n",
      "Training Epoch 3\n",
      "Train on 2500 samples\n",
      "2500/2500 [==============================] - 0s 93us/sample - loss: 5.8435 - accuracy: 0.7520\n"
     ]
    }
   ],
   "source": [
    "trigger_test_inds = np.where(np.all(y_test == source, axis=1))[0]\n",
    "\n",
    "test_poisoned_samples, test_poisoned_labels  = backdoor.poison(x_test[trigger_test_inds], y_test[trigger_test_inds])\n",
    "\n",
    "\n",
    "\n",
    "for i in range(4):\n",
    "    predictions = finetune_classifier.predict(x_test)\n",
    "    accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1)) / len(y_test)\n",
    "    print(\"Accuracy on benign test examples: {}%\".format(accuracy * 100))\n",
    "    \n",
    "    predictions = finetune_classifier.predict(x_test[trigger_test_inds])\n",
    "    b_accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(y_test[trigger_test_inds], axis=1)) / len(trigger_test_inds)\n",
    "    print(\"Accuracy on benign trigger test examples: {}%\".format(b_accuracy * 100))\n",
    "    \n",
    "    predictions = finetune_classifier.predict(test_poisoned_samples)\n",
    "    p_accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(test_poisoned_labels,axis=1)) / len(test_poisoned_labels)\n",
    "    print(\"Accuracy on poison trigger test examples: {}%\".format(p_accuracy * 100))\n",
    "    p_success = np.sum(np.argmax(predictions, axis=1) == np.argmax(target)) / len(test_poisoned_labels)\n",
    "    print(\"Success on poison trigger test examples: {}%\".format(p_success * 100))\n",
    "    print()\n",
    "    print(\"Training Epoch\", i)\n",
    "    if i != 0:\n",
    "        lr *= 0.1\n",
    "        optimizer = tf.keras.optimizers.Adam(learning_rate=lr)\n",
    "        #optimizer = tf.keras.optimizers.SGD(learning_rate=lr, momentum=0.9)\n",
    "        finetune_model.compile(loss=losses.CategoricalCrossentropy(from_logits=True), optimizer=optimizer, metrics=[\"accuracy\"])\n",
    "    finetune_classifier = KerasClassifier(clip_values=(min_, max_), model=finetune_model, use_logits=True)\n",
    "\n",
    "    finetune_classifier.fit(poison_x, poison_y, nb_epochs=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Performance\n",
      "Accuracy on benign test examples: 75.19%\n",
      "Accuracy on benign trigger test examples: 55.60000000000001%\n",
      "Accuracy on poison trigger test examples: 35.099999999999994%\n",
      "Success on poison trigger test examples: 28.9%\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Performance\")\n",
    "predictions = finetune_classifier.predict(x_test)\n",
    "accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(y_test, axis=1)) / len(y_test)\n",
    "print(\"Accuracy on benign test examples: {}%\".format(accuracy * 100))\n",
    "\n",
    "predictions = finetune_classifier.predict(x_test[trigger_test_inds])\n",
    "b_accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(y_test[trigger_test_inds], axis=1)) / len(trigger_test_inds)\n",
    "print(\"Accuracy on benign trigger test examples: {}%\".format(b_accuracy * 100))\n",
    "\n",
    "predictions = finetune_classifier.predict(test_poisoned_samples)\n",
    "p_accuracy = np.sum(np.argmax(predictions, axis=1) == np.argmax(test_poisoned_labels,axis=1)) / len(test_poisoned_labels)\n",
    "print(\"Accuracy on poison trigger test examples: {}%\".format(p_accuracy * 100))\n",
    "p_success = np.sum(np.argmax(predictions, axis=1) == np.argmax(target)) / len(test_poisoned_labels)\n",
    "print(\"Success on poison trigger test examples: {}%\".format(p_success * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
